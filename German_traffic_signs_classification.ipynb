{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings:\n",
    "   <li> The accuracy of the model was 3%! This was fixed by scaling the pixel values between 0-1.\n",
    "   <li> How to save and load models onto the disk (see the last cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NlFsh98IzoZY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vismay/Vismay/Softwares/Anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAD_n0G31_8X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vismay/Vismay/Learning/ML Stuff/Projects/German Road Signs/Dataset/German Traffic Signs/gtsrb-german-traffic-sign\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./../Dataset/'German Traffic Signs'/gtsrb-german-traffic-sign/\"\n",
    "train_csv = \"Train.csv\"\n",
    "test_csv = \"Test.csv\"\n",
    "\n",
    "%cd $dataset_path\n",
    "train_dataset = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XodaqGzW5A9t"
   },
   "source": [
    "### <center>Plotting the frequency distribution of the classes (i.e types of road signs) </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "colab_type": "code",
    "id": "Rcibq9X02Nxy",
    "outputId": "b9a5fb30-dba9-4afd-83b7-2077b3f2dd3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Count')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJwAAAHYCAYAAAAI3/IBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XvYZFV9L/jvDxppFUTkfmhio5IDKiPB9nISNAgZxWCON4jxjApGIUYkITHGnmPmoUeT2CYqHB3UQQXBHMFLvBBBDHJRcozaDQEabRRE0FZBREUzxhus+aN2a/HS3fTbrHpv/fk8Tz21a9Xe+7d2vfXW5Vtr712ttQAAAABAL9vMdgcAAAAAWFgETgAAAAB0JXACAAAAoCuBEwAAAABdCZwAAAAA6ErgBAAAAEBXAicAAAAAuhI4AQAAANCVwAkAAACArhbNdgcmYdddd21Lly6d7W4AAAAALBhXXHHFd1tru23OvAsycFq6dGlWr149290AAAAAWDCq6ubNndcudQAAAAB0JXACAAAAoCuBEwAAAABdLchjOAEAAADcm5///OdZt25dfvKTn8x2V+aUxYsXZ8mSJdluu+22eB0CJwAAAGCrtG7duuy4445ZunRpqmq2uzMntNZy++23Z926ddl33323eD12qQMAAAC2Sj/5yU+yyy67CJvGVFV22WWX+zzqS+AEAAAAbLWETffU4zEROAEAAADQlWM4AQAAACRZuvz8ruu7aeWR9zrPLbfckpNOOimrVq3K9ttvn6VLl+bUU0/Nc57znFx77bVd+nHsscfmGc94Ro466qi7tV922WV54xvfmI9//ONd6owTOAEAAADMgtZanv3sZ+eYY47JueeemyS56qqrcuutt85yz+47u9QBAAAAzIJLL7002223XV72spf9su2ggw7KPvvs88vbN910U570pCfl4IMPzsEHH5zPfvazSZJvf/vbefKTn5yDDjooj370o3P55ZfnzjvvzLHHHptHP/rROfDAA3PKKafco+aFF16Y/fffP4ccckg+/OEPT2zbjHACAAAAmAXXXnttHvvYx25ynt133z0XXXRRFi9enOuvvz7Pf/7zs3r16rzvfe/L0572tLzmNa/JnXfemR//+Me56qqr8s1vfvOXu+L94Ac/uNu6fvKTn+S4447LJZdckkc84hF53vOeN7FtM8IJAAAAYI76+c9/nuOOOy4HHnhgjj766HzpS19KkjzucY/LmWeemRUrVmTNmjXZcccd87CHPSw33nhjTjzxxFx44YV50IMedLd1XXfdddl3332z3377paryghe8YGL9FjgBAAAAzIJHPepRueKKKzY5zymnnJI99tgjV199dVavXp2f/exnSZInP/nJ+cxnPpO99947L3zhC3P22Wdn5513ztVXX51DDz00p512Wl760pfeY31VNZFtmUrgBAAAADALDjvssPz0pz/NO9/5zl+2rVq1KjfffPMvb99xxx3Za6+9ss022+S9731v7rzzziTJzTffnN133z3HHXdcXvKSl+TKK6/Md7/73dx111157nOfm9e97nW58sor71Zv//33z9e+9rV89atfTZKcc845E9s2x3ACAAAASHLTyiNntF5V5SMf+UhOOumkrFy5MosXL87SpUtz6qmn/nKel7/85Xnuc5+bD37wg3nKU56SBz7wgUmSyy67LH//93+f7bbbLjvssEPOPvvsfPOb38yLX/zi3HXXXUmS17/+9Xert3jx4px++uk58sgjs+uuu+aQQw755fGeum9ba20iK55Ny5Yta6tXr57tbgAAAABz2Nq1a3PAAQfMdjfmpA09NlV1RWtt2eYsb5c6AAAAALoSOAEAAADQlWM4zXMHnnXgtJdZc8yaCfQEAAAA5p/W2oyduW2+6HH4JSOcAAAAgK3S4sWLc/vtt3cJWBaK1lpuv/32LF68+D6txwgnAAAAYKu0ZMmSrFu3Lrfddttsd2VOWbx4cZYsWXKf1iFwAgAAALZK2223Xfbdd9/Z7saCZJc6AAAAALoywmlSVuy0Bcvc0b8fAAAAADPMCCcAAAAAuhI4AQAAANCVwAkAAACArgROAAAAAHQlcAIAAACgK4ETAAAAAF0JnAAAAADoSuAEAAAAQFcCJwAAAAC6EjgBAAAA0NWi2e4AAPPHgWcdOO1l1hyzZgI9AQAA5jIjnAAAAADoSuAEAAAAQFcCJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAV4tmuwMAdLBipy1Y5o7+/QAAAIgRTgAAAAB0JnACAAAAoCuBEwAAAABdCZwAAAAA6ErgBAAAAEBXzlIHAAAAbLE3Pe8Z017mle//+AR6wlxihBMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK4ETgAAAAB0JXACAAAAoKuJBU5VtU9VXVpVa6vqi1X1p0P7Q6rqoqq6frjeeWivqnpLVd1QVddU1cFj6zpmmP/6qjpmUn0GAAAA4L6b5AinXyR5ZWvtgCRPTHJCVT0yyfIkF7fW9kty8XA7SZ6eZL/hcnyStyejgCrJyUmekOTxSU5eH1IBAAAAMPcsmtSKW2vfTvLtYfpHVbU2yd5Jnpnk0GG2s5JcluTVQ/vZrbWW5HNV9eCq2muY96LW2veSpKouSnJEknMm1Xdg4Vu6/PxpL3PTyiMn0BMAAICFZ0aO4VRVS5P8RpLPJ9ljCKPWh1K7D7PtneQbY4utG9o21g4AAADAHDTxwKmqdkjyj0lOaq39cFOzbqCtbaJ9ap3jq2p1Va2+7bbbtqyzAAAAANxnEw2cqmq7jMKm/9la+/DQfOuwq1yG6+8M7euS7DO2+JIk39pE+9201k5vrS1rrS3bbbfd+m4IAAAAAJttkmepqyTvTrK2tfbmsbvOS7L+THPHJPnYWPuLhrPVPTHJHcMud59M8tSq2nk4WPhThzYAAAAA5qCJHTQ8yW8leWGSNVV11dD235OsTPKBqnpJkq8nOXq474Ikv5vkhiQ/TvLiJGmtfa+qXpdk1TDfa9cfQBwAAACAuWeSZ6n7l2z4+EtJcvgG5m9JTtjIus5Icka/3jFXnfayS6a9zAnvOGwCPQEAAAC21IycpQ4AAACArYfACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK4ETgAAAAB0JXACAAAAoCuBEwAAAABdCZwAAAAA6ErgBAAAAEBXAicAAAAAuhI4AQAAANCVwAkAAACArgROAAAAAHQlcAIAAACgK4ETAAAAAF0tmu0OwGx40/OeMe1lXvn+j0+gJwAAALDwGOEEAAAAQFcCJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK4ETgAAAAB0JXACAAAAoCuBEwAAAABdCZwAAAAA6ErgBAAAAEBXAicAAAAAuhI4AQAAANCVwAkAAACArgROAAAAAHQlcAIAAACgK4ETAAAAAF0JnAAAAADoSuAEAAAAQFcCJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK4ETgAAAAB0JXACAAAAoCuBEwAAAABdCZwAAAAA6ErgBAAAAEBXAicAAAAAuhI4AQAAANCVwAkAAACArgROAAAAAHQlcAIAAACgK4ETAAAAAF0JnAAAAADoSuAEAAAAQFcCJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK4ETgAAAAB0NbHAqarOqKrvVNW1Y20rquqbVXXVcPndsfv+z6q6oaq+XFVPG2s/Ymi7oaqWT6q/AAAAAPQxyRFO70lyxAbaT2mtHTRcLkiSqnpkkj9I8qhhmbdV1bZVtW2S05I8Pckjkzx/mBcAAACAOWrRpFbcWvtMVS3dzNmfmeTc1tpPk3ytqm5I8vjhvhtaazcmSVWdO8z7pc7dBQAAAKCT2TiG0yuq6pphl7udh7a9k3xjbJ51Q9vG2gEAAACYoyY2wmkj3p7kdUnacP2mJH+YpDYwb8uGA7G2oRVX1fFJjk+SX/u1X+vRVwAAthJr9z9g2ssccN3aCfQEABaGGR3h1Fq7tbV2Z2vtriTvzK92m1uXZJ+xWZck+dYm2je07tNba8taa8t22223/p0HAAAAYLPMaOBUVXuN3Xx2kvVnsDsvyR9U1fZVtW+S/ZJ8IcmqJPtV1b5Vdb+MDix+3kz2GQAAAIDpmdgudVV1TpJDk+xaVeuSnJzk0Ko6KKPd4m5K8kdJ0lr7YlV9IKODgf8iyQmttTuH9bwiySeTbJvkjNbaFyfVZwAAAADuu0mepe75G2h+9ybm/5skf7OB9guSXNCxawAAAABM0EwfNBzgXu156VXTmv+Wpxw0oZ4AAACwJWb0GE4AAAAALHwCJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdOUsdbAArFixYkaWAQAAgM1hhBMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlbPUsVnW7n/AtJc54Lq1E+gJs+niSx4+7WUOP+yrE+gJAAAAc5kRTgAAAAB0JXACAAAAoCuBEwAAAABdCZwAAAAA6ErgBAAAAEBXAicAAAAAuhI4AQAAANCVwAkAAACArgROAAAAAHQlcAIAAACgK4ETAAAAAF0JnAAAAADoSuAEAAAAQFcCJwAAAAC6EjgBAAAA0NWi2e4AwEK2dPn5017mppVHTqAnAAAAM8cIJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK4ETgAAAAB0tVmBU1X91ua0AQAAAMDmjnB662a2AQAAALCVW7SpO6vqvyT5zSS7VdWfj931oCTbTrJjAAAAAMxPmwycktwvyQ7DfDuOtf8wyVGT6hQAAAAA89cmA6fW2qeTfLqq3tNau3mG+gQAAADAPHZvI5zW276qTk+ydHyZ1tphk+gUAAAAAPPX5gZOH0zyjiTvSnLn5LoDAAAAwHy3uYHTL1prb59oTwAAAABYELbZzPn+qapeXlV7VdVD1l8m2jMAAAAA5qXNHeF0zHD9qrG2luRhfbsDAMx1K1asmJFlAGCmHXjWgdNeZs0xaybQE5j/Nitwaq3tO+mOAAAAALAwbFbgVFUv2lB7a+3svt0BAAAAYL7b3F3qHjc2vTjJ4UmuTCJwAgAAAOBuNneXuhPHb1fVTkneO5EeAQAAADCvbe5Z6qb6cZL9enYEAAAAgIVhc4/h9E8ZnZUuSbZNckCSD0yqUwAAAADMX5t7DKc3jk3/IsnNrbV1E+gPAAAAAPPcZu1S11r7dJLrkuyYZOckP5tkpwAAAACYvzYrcKqq30/yhSRHJ/n9JJ+vqqMm2TEAAAAA5qfN3aXuNUke11r7TpJU1W5JPpXkQ5PqGAAAAADz0+aepW6b9WHT4PZpLAsAAADAVmRzRzhdWFWfTHLOcPt5SS6YTJcAAAAAmM82GThV1SOS7NFae1VVPSfJIUkqyb8m+Z8z0D8AAAAA5pl72y3u1CQ/SpLW2odba3/eWvuzjEY3nTrpzgEAAAAw/9xb4LS0tXbN1MbW2uokSyfSIwAAAADmtXsLnBZv4r779+wIAAAAAAvDvR00fFVVHddae+d4Y1W9JMkVk+sWADBd65ZfPu1llqx80gR6AgDA1u7eAqeTknykqv6P/CpgWpbkfkmePcmOAQAAADA/bTJwaq3dmuQ3q+opSR49NJ/fWrtk4j0DAAAAYF66txFOSZLW2qVJLp1wXwAAAABYAO7toOEAAAAAMC0CJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK4ETgAAAAB0NbHAqarOqKrvVNW1Y20PqaqLqur64Xrnob2q6i1VdUNVXVNVB48tc8ww//VVdcyk+gsAAABAH5Mc4fSeJEdMaVue5OLW2n5JLh5uJ8nTk+w3XI5P8vZkFFAlOTnJE5I8PsnJ60MqAAAAAOamiQVOrbXPJPnelOZnJjlrmD4rybPG2s9uI59L8uCq2ivJ05Jc1Fr7Xmvt+0kuyj1DLAAAAADmkJk+htMerbVvJ8lwvfvQvneSb4zNt25o21j7PVTV8VW1uqpW33bbbd07DgAAAMDmmSsHDa8NtLVNtN+zsbXTW2vLWmvLdtttt66dAwAAAGDzzXTgdOuwq1yG6+8M7euS7DM235Ik39pEOwAAAABz1EwHTuclWX+muWOSfGys/UXD2eqemOSOYZe7TyZ5alXtPBws/KlDGwAAAABz1KJJrbiqzklyaJJdq2pdRmebW5nkA1X1kiRfT3L0MPsFSX43yQ1JfpzkxUnSWvteVb0uyaphvte21qYeiBwAAACAOWRigVNr7fkbuevwDczbkpywkfWckeSMjl0DAAAAYILmykHDAQAAAFggBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK4ETgAAAAB0JXACAAAAoCuBEwAAAABdCZwAAAAA6ErgBAAAAEBXAicAAAAAuhI4AQAAANCVwAkAAACArgROAAAAAHQlcAIAAACgK4ETAAAAAF0JnAAAAADoSuAEAAAAQFcCJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK4ETgAAAAB0tWi2OwAAU63d/4BpL3PAdWsn0BMA6GvPS6+a9jK3POWgCfQEYLKMcAIAAACgK4ETAAAAAF0JnAAAAADoSuAEAAAAQFcCJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK4ETgAAAAB0tWi2OwAAAMD8tHT5+dNe5qaVR06gJ8BcY4QTAAAAAF0JnAAAAADoSuAEAAAAQFcCJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhq0Wx3AABmy2kvu2Tay5zwjsMm0BMAAFhYjHACAAAAoCuBEwAAAABdCZwAAAAA6ErgBAAAAEBXAicAAAAAunKWOgAAmCHOjgnA1sIIJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQ1aLZ7gAAADA/rVt++bSXWbLySRPoCQBzjRFOAAAAAHQlcAIAAACgK4ETAAAAAF3NSuBUVTdV1ZqquqqqVg9tD6mqi6rq+uF656G9quotVXVDVV1TVQfPRp8BAAAA2DyzedDwp7TWvjt2e3mSi1trK6tq+XD71UmenmS/4fKEJG8frgEAmC0rdtqCZe7o3w8AYE6aS7vUPTPJWcP0WUmeNdZ+dhv5XJIHV9Ves9FBAAAAAO7dbAVOLck/V9UVVXX80LZHa+3bSTJc7z60753kG2PLrhva7qaqjq+q1VW1+rbbbptg1wEAAADYlNnape63Wmvfqqrdk1xUVddtYt7aQFu7R0Nrpyc5PUmWLVt2j/sBAAAAmBmzMsKptfat4fo7ST6S5PFJbl2/q9xw/Z1h9nVJ9hlbfEmSb81cbwEAAACYjhkPnKrqgVW14/rpJE9Ncm2S85IcM8x2TJKPDdPnJXnRcLa6Jya5Y/2udwAAAADMPbOxS90eST5SVevrv6+1dmFVrUrygap6SZKvJzl6mP+CJL+b5IYkP07y4pnvMgAAAACba8YDp9bajUkes4H225McvoH2luSEGegaAAAAAB3M1lnqAAAAAFigBE4AAAAAdCVwAgAAAKCr2ThoOAAAwJxz8SUPn/Yyhx/21Qn0BGD+M8IJAAAAgK4ETgAAAAB0JXACAAAAoCuBEwAAAABdCZwAAAAA6MpZ6mCC1i2/fNrLLFn5pAn0BJhNb3reM6a9zCvf//EJ9AQA5qkVO23BMnf07wew2YxwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVw4aDgCwgCxdfv60l7lp5ZET6AkAG7N2/wOmvcwB162dQE9gcoxwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALpyljoAYKu256VXTXuZW55y0AR6AgDMBStWrJiRZRY6I5wAAAAA6ErgBAAAAEBXAicAAAAAuhI4AQAAANCVg4YDAMyApcvPn/YyN608cgI9AQCYPCOcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK6cpQ4AAACY89Ytv3zayyxZ+aQJ9ITNYYQTAAAAAF0JnAAAAADoSuAEAAAAQFcCJwAAAAC6ctBwAADmrAPPOnDay6w5Zs0EesJsWrFixYwsA0A/RjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXTlLHQAALDBvet4zpr3MK9//8Qn0hNmydPn5017mppVHTqAnwMZcfMnDp73M4Yd9ddrL7HnpVdNe5panHDTtZaYywgkAAACArgROAAAAAHQlcAIAAACgK4ETAAAAAF1tdQcNd/A8AJgfZupAmgCwUJ32skumvcwJ7zhsAj1ha2SEEwAAAABdCZwAAAAA6ErgBAAAAEBXAicAAAAAuhI4AQAAANCVwAkAAACArgROAAAAAHQlcAIAAACgK4ETAAAAAF0JnAAAAADoSuAEAAAAQFcCJwAAAAC6EjgBAAAA0JXACQAAAICuBE4AAAAAdCVwAgAAAKArgRMAAAAAXQmcAAAAAOhK4AQAAABAVwInAAAAALoSOAEAAADQlcAJAAAAgK4ETgAAAAB0JXACAAAAoCuBEwAAAABdCZwAAAAA6ErgBAAAAEBXAicAAAAAuhI4AQAAANCVwAkAAACArgROAAAAAHQlcAIAAACgK4ETAAAAAF0JnAAAAADoSuAEAAAAQFfzJnCqqiOq6stVdUNVLZ/t/gAAAACwYfMicKqqbZOcluTpSR6Z5PlV9cjZ7RUAAAAAGzIvAqckj09yQ2vtxtbaz5Kcm+SZs9wnAAAAADagWmuz3Yd7VVVHJTmitfbS4fYLkzyhtfaKsXmOT3L8cPM/J/nyNMvsmuS7HbqrjjrqqKOOOuosjDoLaVvUUUcdddRRRx11etR4aGttt82ZcdH0+zMragNtd0vKWmunJzl9iwtUrW6tLdvS5dVRRx111FFHnYVVZyFtizrqqKOOOuqoo85M15gvu9StS7LP2O0lSb41S30BAAAAYBPmS+C0Ksl+VbVvVd0vyR8kOW+W+wQAAADABsyLXepaa7+oqlck+WSSbZOc0Vr7YucyW7w7njrqqKOOOuqosyDrLKRtUUcdddRRRx111JnRGvPioOEAAAAAzB/zZZc6AAAAAOYJgRMAAAAAXQmcAAAAAOhqXhw0fBKqav8kz0yyd5KW5FtJzmutrZ3Vjm2hYXv2TvL51tq/j7Uf0Vq7sGOdxydprbVVVfXIJEckua61dkGvGhupe3Zr7UUTrnFIkscnuba19s8d1/uEJGtbaz+sqvsnWZ7k4CRfSvK3rbU7OtX5kyQfaa19o8f6NlFn/Zkiv9Va+1RV/bckv5lkbZLTW2s/71jr4UmenWSfJL9Icn2Sc3o9ZgDMX1W1e2vtO7Pdj16qapfW2u2z3Q8A6GWrHOFUVa9Ocm6SSvKFJKuG6XOqavkM9uPFndbzJ0k+luTEJNdW1TPH7v7bHjWGOicneUuSt1fV65P8P0l2SLK8ql7Tsc55Uy7/lOQ56293rPOFsenjMtqeHZOc3Pl5cEaSHw/T/yPJTkneMLSd2bHO65J8vqour6qXV9VuHdc97swkRyb506p6b5Kjk3w+yeOSvKtXkeF5/Y4ki4d13z+j4Olfq+rQXnWYf6pq99nuQ09Vtcts94GkqnaqqpVVdV1V3T5c1g5tD56hPnyi47oeVFWvr6r3Dj8MjN/3to519qyqt1fVaVW1S1WtqKo1VfWBqtqrY52HTLnskuQLVbVzVT2kY50jxqZ3qqp3V9U1VfW+qtqjY52VVbXrML2sqm7M6D385qr67Y51rqyqvxp+wJmYYRsurap/qKp9quqiqrqjqlZV1W90rLNDVb22qr44rP+2qvpcVR3bq8ZQZ1FV/VFVXTj8/a+uqk9U1cuqaruetTbRh25njqqqbYfteV1V/daU+/6qY50HVNVfVtWrqmpxVR07fH7/u6raoVedjdT+ygTW+b+NTW83/C+dV1V/W1UP6FTjFWOvBY+oqs9U1Q+q6vNVdWCPGsO6P1xVL5iBv8PDquqMqvrr4f/1nVV1bVV9sKqWdqyzTVX9YVWdP/x/XlFV5/b+juC1oIPW2lZ3SfKVJNttoP1+Sa6fwX58vdN61iTZYZhemmR1kj8dbv9bx/6uSbJtkgck+WGSBw3t909yTcc6Vyb5hySHJvnt4frbw/Rvd6zzb2PTq5LsNkw/MMmajnXWjm/blPuu6rk9GYXIT03y7iS3JbkwyTFJduxY55rhelGSW5NsO9yuzs+DNWPrfkCSy4bpX+v8vN4pycok1yW5fbisHdoe3KvOvfThEx3X9aAkr0/y3iT/bcp9b+tYZ88kb09yWpJdkqwY/mYfSLJXxzoPmXLZJclNSXZO8pCOdY6Y8px4d5JrkrwvyR7oy8OvAAALZUlEQVQd66xMsuswvSzJjUluSHJz59e3K5P8VZKH91rnRuosS3Lp8Jq9T5KLktwxvKb+Rsc6OyR5bZIvDuu/LcnnkhzbscYnk7w6yZ5jbXsObRd1rHPwRi6PTfLtjnX+cXi+PSvJecPt7dc/PzrWuTCjH7yWD/8zrx5ep09M8rGOde5K8rUpl58P1zd2rHPl2PS7kvx1kocm+bMkH+1YZ83Y9KVJHjdM/3qS1R3rfC3JG5N8PaMfWf8syX/qtf6xOl9I8vQkz0/yjSRHDe2HJ/nXjnU+luTYJEuS/HmS/yvJfknOymjUeK8652T0HvfEodaSYfrtSd7fsc7U97jx97p1Heu8K6P3s5OSXJHkzWP39Xw9+ECSNyV5W5KLM/ox98lJ/j7JezvW+VFG30V+OEz/KMmd69s71hl/PXhTkvdk9H3klCRnd6rxxbHp85M8e5g+NMn/6rgt30zyoSTfG/5Oz05yv17rH6vzmSR/nNF7wrVJXpnR54OXJLmkY50zM/rseUiSUzP6jPC/J/lUkhM71vFacF9rTmKlc/2S0RfLh26g/aFJvty51jUbuaxJ8tNONb405fYOGX0AfHM6Bxobmh5u96yzTUYfiC5KctDQ1u3D5FidqzP60rpLpny4m7p997HOB5O8eJg+M8myYfrXk6zqWGdqmLVdkv86vFDe1rHOtRmFsztn9Mb+kKF9ccbCtQ511uRXX452TnLFeB861vElc8vq+JK5ZXV8ydyyOhP/kplNvP9v6r4tqHNnkkuGv//Uy390rHPVlNuvSfK/MnrP6/laMP7Z4Oub6sN9rPMXw+vOgWNtX+u1/rF1jr8WTH0Me27PdUkWDdOfm3Jfzx+9xrfnSRkFAbcMz7fjZ+h50PMz1dVTbq8arrfJ6BAPveps6vXgKx3r3JnRDw/j73Hrb/+sY51rxqYXJTk9yYeTbN/573PVcF3D86zGbvf8UfKtSc7O2A9CE3o9GH9eX5VhwELP7Rl/rmXK94LOj9m/Ddc7Jnlhkgsy+vHmzCRPndBjNsnXgmum3P7ccL19+n4f8VpwHy9b6zGcTkpycVVdn9EH5GT0RekRSV7RudYeSZ6W5PtT2ivJZzvVuKWqDmqtXZUkrbV/r6pnZLQrV7ehmEl+VlUPaK39OKMvyUlGQ84z+lLYRWvtriSnVNUHh+tbM5njje2UUbJbSVpV7dlau2UYalod67w0yf8Yhil+N6Ndwr6R0XPvpR3r3K3PbXQspfOSnFejY0f18u6MPihvm9EXmA8OuwI8MaNdVXt5V5JVVfW5jH4de0OS1GhXwe91rLO0tfaG8YbW2i1J3lBVf9ixzqokn86Gn1s9d9d5eGvtucP0R2u0u+slVfVfO9ZIRh/y3pokVfXyscfwrVX1ko51/jLJ7yR5VWttzVDva621fTvWmGpZa+2gYfqUqjqm47q3q6pFrbVfJLl/a21VkrTWvlJV23es8/3W2l8k+YuqelJGgdCVVbU2o+Og9RqevV1r7RNJUlVvaK19KElaaxdX1Rs71UhG/6fvGabfXFWrWmuvq9Gu6V9K8t871Li5qv4yyVmttVuTZNiF6tj86rNCD2uT/FFr7fqpdwzvDb1sX1XbDO+paa39TVWty+jX5567VIwfnuHsKfdt26tIa+2NVXVuRv+T30hyckbH4Oxt96r684xeqx9UVdWGT+bpeyiK05JcUFUrk1xYVadm9KH/8Iy+2HbXWrs8yeVVdWJGIwGel9GXjR5+UlVPzeizVauqZ7XWPlqj3QPv7FQjSf6/qjqktfYvVfV7GT4PtNbuqqqen92+X1VHJ/nH9f9DVbVNRocRmPqZ/r64McnhrbWvT72j8+vB/dZPDO8/x9foUBmXpO/rwfoaraouWP+/M9zu9v/aWjuxqh6b0eFQPprRSKpJvB7sVFXPyej1YPvhs3Xv7flQVb0noxE6H6mqk/Kr14J7PC/ug/V/ix9lNAr+vTXaHfn3M/rxsNfxa++qql/P6LPtA6pqWWttdVU9Ih3fE5L8vKoe3lr7alUdnORnSdJa+2nP51q8Ftx3k0ix5sMlow8NT0zy3CRHDdPbTqDOu5McspH73tepxpKMjc6Yct9vddyW7TfSvmvGfnGcwGN4ZDoOk96Meg9Isu8E1rtjksdkFNZ120VnbP2/PoOP0X/KMFoiozeUo5I8fgJ1HjWse/8Jbss/ZxRqjP9KtkdGI3Y+1bHOtUn228h93+hYZ22Sbaa0HZPRrkg3d6xz9dj0X0+5r9uv88P6lmQ0UvDNw//RJEY8rsto5MwrM3rTr7H7ev7CeOLwnDsso6Hgp2YUqP7f6bu7wT1GsGT0Qe+IJGd2rPOvGe3Ge3RGuwU+a2j/7fQdsfXZ9e+lSX4vySfH7usy+iijkZRvyChQ/35GX2TXDm09d988Ksl/3sh9z+pY5++S/M4G2o9Ix8MHZPQlaYcNtD8iyYd61Zmy7t/LaJfKWyaw7pOnXNbvbr9nOu1CM1br0CTvz2iX+DUZjTg4Phs47MN9qHHuJP4GG6jzmIxGDH8iyf4ZHbPyB8N7z292rvOFYd3/sv5/KcluSf6kY52lw9/mOxkdiuMrw/T70/EzYpITkjxmI/f13C3oHzK26/hY+0uT/LxjnXdt5PXg4Un+ZQLPu22S/EmSyzM6mU3v9Z855bLH0L5nkos71jk2o+OhfjejvQe+lNFxeHfqWOMzvR+fjdQ5PMmXh/fPQzIaaX/98P/zzI51DssokPtKRqOAnjC075bk7zrWWf9acNtQa/22eC3YzMv6YY4AW6Wq2jmjX3aemWT9wahvzWh02MrWWpdfL6rqqIyCmC9v4L5ntdY+2qnO3yX559bap6a0H5Hkra21/TrVeW1Gb+j/PqX9ERk9bkf1qDNl3b+X0ai6pa21PTuv++QpTW9rrd1WVXtmtJ3dzpI5HNDyjzPajW5RRqNnPprkjDb6talHjXNba3/QY133UucxGQUbd2W0294fZxRwfjPJca21LiN5a3Tg1ndl9Jhdm+QP22hU2G5Jnt9ae0unOvtnFHB+rk32jK8zdWbZjdV5ehtGpk24zsS2J6NRMw9vrV27AP8+87XOARn9IDUTdfbO5P9Pn5DRqJCvJjkgox+nv9Q6n5m5ZugM0LNc58tJfjniaQJ1npTkKRn90NF7e56Q5K5JPm5TtuVRQ4218/g5MP6YPSqjXe8n8b/zX5L8YtLbM1Zvl4xGu53aWnvBJGpMqTfxs7TPRB2BE8BGVNWLW2tnqjN36gy7h67/kjnvt0eduVOnRmfGPCGjX2UPyujkGx8b7ruytXbwfa0xw3VOzOgwAQulzkJ73Bbi9rw8oxGCC6HOyRl9SV6U0TFFH5/RbvG/k9EIy7+ZUJ0nJLlMnWnXmam/T/c6W8Fzbd7+bYY6GzpD+mEZ7YKW1lqXQ1ZsoE5lFKLOyzp302uolIuLi8tCu6TTmSTVUUeduV8nM3vGV3XUUWfu15mJMzOrs5XXWUjbskDrzNjZ0xdSnfHL1nrQcIAkSVVds7G7MjqWkzrqqLN11Nm2DbvntNZuGnZ9/FBVPTR9TyShjjrqzP06v2it3Znkx1X11dbaD4ea/1FV3U6Uo446M1RDnS23LMmfZnRIh1e11q6qqv9orX26Y41kdIzfhVTnlwROwNZuJs4kqY466sz9OjN1xld11FFn7teZkTMzq6PODNVQZwu1GTp7+kKrM07gBGztPp7R8Px7nIa6qi5TRx11tpo6L0pyt4O2t9FB3F9UVf9vpxrqqKPO/Kjz5NbaT4f1j3953S6jEyOoo06vOgtpWxZinQw11iU5uqqOzGgXvolYaHUSBw0HAAAAoLNtZrsDAAAAACwsAicAAAAAuhI4AQAAANCVwAkAAACArgROAAAAAHT1/wMtD5DL30ut5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_frequency = train_dataset['ClassId'].value_counts().sort_index()\n",
    "class_frequency.plot(kind='bar', legend=True, figsize=(20,8)).set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ygQ0WX8X4Et1",
    "outputId": "37424ff1-0052-4703-b21c-e11a94cd8b28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Width', 'Height', 'Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'ClassId',\n",
       "       'Path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "KfvzQoRD8-uO",
    "outputId": "5e592f54-412c-4951-a322-47f67dfb6c5d"
   },
   "outputs": [],
   "source": [
    "# Load images from the dataset and then:\n",
    "#   1. Resize to 32x32 pixels\n",
    "#   2. Convert RGB to greyscale\n",
    "#   3. Use histogram equalization to improve the contrast of the image\n",
    "#   4. Scale the pixel values betwenn 0 to 1\n",
    "#   5. Store the corresponding labels\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "image_dim = (32,32)\n",
    "no_of_images = train_dataset['Path'].shape[0]\n",
    "\n",
    "for i in range(no_of_images):\n",
    "    try:\n",
    "        img = cv2.imread(train_dataset['Path'][i])\n",
    "        img_resized = cv2.resize(img, image_dim)\n",
    "        img_grayscale = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "        img_hist_equalized = cv2.equalizeHist(img_grayscale)\n",
    "        img_scaled = img_hist_equalized/255.0\n",
    "        # AAARRRGGGHHHH train_images.append(img_hist_equalized)\n",
    "        train_images.append(img_scaled)\n",
    "        train_labels.append(train_dataset['ClassId'][i])\n",
    "    except Exception as e:\n",
    "        print('Index: ' + i + ' ' + str(e))\n",
    "        \n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6hMPiPUvRy65",
    "outputId": "edc8494b-58ee-4e29-9b74-6ed4d0a25a71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataset: (39209, 32, 32), (39209,)\n",
      "The classes of images in the current dataset are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dimensions of the dataset: {train_images.shape}, {train_labels.shape}\")\n",
    "print(f\"The classes of images in the current dataset are {np.unique(train_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "nJB7wHvHxY2z",
    "outputId": "97df37ae-597d-483e-dc07-7a0c0bc14b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: Vertical flip - (2220, 32, 32)\n",
      "Assigning class 1\n",
      "Class 5: Vertical flip - (1860, 32, 32)\n",
      "Assigning class 5\n",
      "Class 11: Horizontal self flip - (1320, 32, 32)\n",
      "Assigning class 11\n",
      "Class 12: Horizontal self flip - (2100, 32, 32)\n",
      "Assigning class 12\n",
      "Class 12: Vertical flip - (2100, 32, 32)\n",
      "Assigning class 12\n",
      "Class 13: Horizontal self flip - (2160, 32, 32)\n",
      "Assigning class 13\n",
      "Class 15: Horizontal self flip - (630, 32, 32)\n",
      "Assigning class 15\n",
      "Class 15: Vertical flip - (630, 32, 32)\n",
      "Assigning class 15\n",
      "Class 17: Horizontal self flip - (1110, 32, 32)\n",
      "Assigning class 17\n",
      "Class 17: Vertical flip - (1110, 32, 32)\n",
      "Assigning class 17\n",
      "Class 18: Horizontal self flip - (1200, 32, 32)\n",
      "Assigning class 18\n",
      "Class 19: Horizontal cross flip - (210, 32, 32)\n",
      "Assigning class 20\n",
      "Class 20: Horizontal cross flip - (360, 32, 32)\n",
      "Assigning class 19\n",
      "Class 22: Horizontal self flip - (390, 32, 32)\n",
      "Assigning class 22\n",
      "Class 26: Horizontal self flip - (600, 32, 32)\n",
      "Assigning class 26\n",
      "Class 30: Horizontal self flip - (450, 32, 32)\n",
      "Assigning class 30\n",
      "Class 32: V & H flip - (240, 32, 32)\n",
      "Assigning class 32\n",
      "Class 33: Horizontal cross flip - (689, 32, 32)\n",
      "Assigning class 34\n",
      "Class 34: Horizontal cross flip - (420, 32, 32)\n",
      "Assigning class 33\n",
      "Class 35: Horizontal self flip - (1200, 32, 32)\n",
      "Assigning class 35\n",
      "Class 36: Horizontal cross flip - (390, 32, 32)\n",
      "Assigning class 37\n",
      "Class 37: Horizontal cross flip - (210, 32, 32)\n",
      "Assigning class 36\n",
      "Class 38: Horizontal cross flip - (2070, 32, 32)\n",
      "Assigning class 39\n",
      "Class 39: Horizontal cross flip - (300, 32, 32)\n",
      "Assigning class 38\n",
      "Class 40: V & H flip - (360, 32, 32)\n",
      "Assigning class 40\n",
      "Dimensions of flipped images: (24329, 32, 32) and labels: (24329,)\n"
     ]
    }
   ],
   "source": [
    "# By observing the various road signs in Meta, it can be observed that certain road signs can be flipped/\n",
    "# inverted or rotated to get other road signs, or the same road signs. This augmentation technique can be\n",
    "# used to balance the dataset, i.e add more samples for classes that have a small number of samples.\n",
    "\n",
    "def flip_images(images_to_flip, source_label, target_label, flip_orientation):\n",
    "    flipped_images = []\n",
    "    flipped_img_labels = []\n",
    "    for img in range(images_to_flip.shape[0]):\n",
    "        original_img = images_to_flip[img]\n",
    "        #print(f\"Type: {type(original_img)} and shape before flipping: {original_img.shape}\")\n",
    "        flipped_img = cv2.flip(original_img, flip_orientation)\n",
    "        #print(f\"Type: {type(flipped_img)} and shape after flipping: {flipped_img.shape}\")\n",
    "        flipped_images.append(flipped_img)\n",
    "        flipped_img_labels.append(target_label)\n",
    "    flipped_images = np.array(flipped_images)\n",
    "    flipped_img_labels = np.array(flipped_img_labels)\n",
    "    print(f\"Assigning class {target_label}\")\n",
    "    return {'flipped_images':flipped_images, 'labels':flipped_img_labels}\n",
    "\n",
    "# Road signs which, when flipped horizontally give the same sign. Example: class 11\n",
    "horizontally_self_flippable_signs = [11, 12, 13, 15, 17, 18, 22, 26, 30, 35]\n",
    "\n",
    "# Road signs which, when flipped vertically give the same sign. Example: Class 1\n",
    "vertically_self_flippable_signs = [1, 5, 12, 15, 17]\n",
    "\n",
    "# Road signs which, when flipped horizontally, then vertically give the same sign. Example: Class 32.\n",
    "self_flippable_both = [32, 40]\n",
    "\n",
    "# Road signs which, when flipped horizontally give another sign from the dataset. Example: Horizontally flipping class 33 gives class 34\n",
    "horizontally_cross_flippable_signs = np.array([[19, 20], [33, 34], [36, 37],  [38, 39], [20, 19],  [34, 33], [37, 36], [39, 38]])\n",
    "\n",
    "num_classes = 43\n",
    "h_flip_self = []\n",
    "v_flip = []\n",
    "both_flip = []\n",
    "h_flip_cross = []\n",
    "flipped_images = np.array([])\n",
    "flipped_img_labels = np.array([])\n",
    "\n",
    "for c in range(num_classes):\n",
    "    if c in np.unique(train_labels):\n",
    "        if c in horizontally_self_flippable_signs:\n",
    "            images_to_flip = train_images[train_labels==c]\n",
    "            print(f\"Class {c}: Horizontal self flip - {images_to_flip.shape}\")\n",
    "            h_flip_self = flip_images(images_to_flip, c, c, 1)\n",
    "            if flipped_images.size == 0:\n",
    "                flipped_images = h_flip_self['flipped_images']\n",
    "                flipped_img_labels = h_flip_self['labels']\n",
    "            else:\n",
    "                flipped_images = np.append(flipped_images, h_flip_self['flipped_images'], axis=0)\n",
    "                flipped_img_labels = np.append(flipped_img_labels, h_flip_self['labels'], axis=0)\n",
    "        \n",
    "        if c in vertically_self_flippable_signs:\n",
    "            images_to_flip = train_images[train_labels==c]\n",
    "            print(f\"Class {c}: Vertical flip - {images_to_flip.shape}\")\n",
    "            v_flip = flip_images(images_to_flip, c, c, 0)\n",
    "            if flipped_images.size == 0:\n",
    "                flipped_images = v_flip['flipped_images']\n",
    "                flipped_img_labels = v_flip['labels']\n",
    "            else:\n",
    "                flipped_images = np.append(flipped_images, v_flip['flipped_images'], axis=0)\n",
    "                flipped_img_labels = np.append(flipped_img_labels, v_flip['labels'], axis=0)\n",
    "            \n",
    "        if c in self_flippable_both:\n",
    "            images_to_flip = train_images[train_labels==c]\n",
    "            print(f\"Class {c}: V & H flip - {images_to_flip.shape}\")\n",
    "            both_flip = flip_images(images_to_flip, c, c, -1)\n",
    "            if flipped_images.size == 0:\n",
    "                flipped_images = both_flip['flipped_images']\n",
    "                flipped_img_labels = both_flip['labels']\n",
    "            else:\n",
    "                flipped_images = np.append(flipped_images, both_flip['flipped_images'], axis=0)\n",
    "                flipped_img_labels = np.append(flipped_img_labels, both_flip['labels'], axis=0)\n",
    "                    \n",
    "        if c in horizontally_cross_flippable_signs[:,0]:\n",
    "            source_label = c\n",
    "            target_label_index = np.where(horizontally_cross_flippable_signs[:,0]==c)[0][0]\n",
    "            target_label = horizontally_cross_flippable_signs[target_label_index][1]\n",
    "            images_to_flip = train_images[train_labels==c]\n",
    "            print(f\"Class {c}: Horizontal cross flip - {images_to_flip.shape}\")\n",
    "            h_flip_cross = flip_images(images_to_flip, source_label, target_label, 1)\n",
    "            if flipped_images.size == 0:\n",
    "                flipped_images = h_flip_cross['flipped_images']\n",
    "                flipped_img_labels = h_flip_cross['labels']\n",
    "            else:\n",
    "                flipped_images = np.append(flipped_images, h_flip_cross['flipped_images'], axis=0)\n",
    "                flipped_img_labels = np.append(flipped_img_labels, h_flip_cross['labels'], axis=0)\n",
    "print(f\"Dimensions of flipped images: {flipped_images.shape} and labels: {flipped_img_labels.shape}\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lUhC-eet2ipx",
    "outputId": "b1913c09-0eee-4a87-c8a8-40e82edbf476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataset before adding flipped images: (39209, 32, 32) and labels: (39209,)\n",
      "Dimensions of the dataset after adding flipped images: (63538, 32, 32) and labels: (63538,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dimensions of the dataset before adding flipped images: {train_images.shape} and labels: {train_labels.shape}\")\n",
    "\n",
    "# Append the flipped images to the original dataset\n",
    "train_images = np.append(train_images, flipped_images, axis=0)\n",
    "train_labels = np.append(train_labels, flipped_img_labels, axis=0)\n",
    "print(f\"Dimensions of the dataset after adding flipped images: {train_images.shape} and labels: {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_class = 43\\nbalanced_data = []\\nbalanced_labels = []\\nfor c in range(num_classes):\\n    class_c_signs = np.where(train_labels==c)[0]\\n    sample_count = 200\\n    indices = random.sample(range(0, len(class_c_signs)), sample_count)\\n    for i in range(sample_count):\\n        balanced_data.append(train_images[class_c_signs[i]])\\n        balanced_labels.append(c)\\n\\nbalanced_data = np.array(balanced_data)\\nbalanced_labels = np.array(balanced_labels)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a balanced dataset\n",
    "\"\"\"num_class = 43\n",
    "balanced_data = []\n",
    "balanced_labels = []\n",
    "for c in range(num_classes):\n",
    "    class_c_signs = np.where(train_labels==c)[0]\n",
    "    sample_count = 200\n",
    "    indices = random.sample(range(0, len(class_c_signs)), sample_count)\n",
    "    for i in range(sample_count):\n",
    "        balanced_data.append(train_images[class_c_signs[i]])\n",
    "        balanced_labels.append(c)\n",
    "\n",
    "balanced_data = np.array(balanced_data)\n",
    "balanced_labels = np.array(balanced_labels)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image',train_images[385])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUO09myYuUDO"
   },
   "outputs": [],
   "source": [
    "# Apply additional transformations such as image translation and shearing.\n",
    "# Randomly choose 10% of the images from each class and apply the transformations.\n",
    "\n",
    "# Will be done later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9HzpP8Mp3qG1"
   },
   "outputs": [],
   "source": [
    "# One-hot-encode the image labels\n",
    "number_of_classes = 43\n",
    "train_labels = np.eye(43)[train_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nQzeb1nhPKHO",
    "outputId": "c39d18a4-ae5d-4244-ef3a-db8ae78632c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training data and labels: (50830, 32, 32) (50830, 43)\n",
      "Dimensions of validation data and labels: (12708, 32, 32) (12708, 43)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bal_X_train, bal_X_val, bal_y_train, bal_y_val = train_test_split(balanced_data, balanced_labels, test_size=0.20)\\nprint(f\"Dimensions of training data and labels: {bal_X_train.shape} {bal_y_train.shape}\")\\nprint(f\"Dimensions of validation data and labels: {bal_X_val.shape} {bal_y_val.shape}\")'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use train_test_split() to create a hold out validation dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.20)\n",
    "print(f\"Dimensions of training data and labels: {X_train.shape} {y_train.shape}\")\n",
    "print(f\"Dimensions of validation data and labels: {X_val.shape} {y_val.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"bal_X_train, bal_X_val, bal_y_train, bal_y_val = train_test_split(balanced_data, balanced_labels, test_size=0.20)\n",
    "print(f\"Dimensions of training data and labels: {bal_X_train.shape} {bal_y_train.shape}\")\n",
    "print(f\"Dimensions of validation data and labels: {bal_X_val.shape} {bal_y_val.shape}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PKZHRgf4d8r"
   },
   "outputs": [],
   "source": [
    "# Reshape the data to make it compatible with the input_shape of the CNN\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], X_val.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "SEg2MmzbCPyN",
    "outputId": "0eca36b3-2a79-4504-a76a-fe711f53ed17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 43)                44075     \n",
      "=================================================================\n",
      "Total params: 2,399,275\n",
      "Trainable params: 2,399,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The architecture of the CNN is as described in: http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf\n",
    "\n",
    "\n",
    "# Build the CNN model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "\n",
    "# Conv layer 1\n",
    "conv_model = Sequential()\n",
    "conv_model.add(Conv2D(32, kernel_size=5, input_shape=(32,32,1), padding='same', activation='relu'))\n",
    "conv_model.add(MaxPool2D((2,2)))\n",
    "#conv_model.add(Dropout(0.1))\n",
    "\n",
    "# Conv layer 2\n",
    "conv_model.add(Conv2D(64, kernel_size=5, padding='same', activation='relu'))\n",
    "conv_model.add(MaxPool2D(2,2))\n",
    "#conv_model.add(Dropout(0.2))\n",
    "\n",
    "# Conv layer 3\n",
    "conv_model.add(Conv2D(128, kernel_size=5, padding='same', activation='relu'))\n",
    "conv_model.add(MaxPool2D(2,2))\n",
    "#conv_model.add(Dropout(0.3))\n",
    "\n",
    "# Flatten\n",
    "conv_model.add(Flatten())\n",
    "#conv_model.add(Dropout(0.5))\n",
    "\n",
    "# Fully connected layer 1:\n",
    "conv_model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "conv_model.add(Dense(43, activation='softmax'))\n",
    "\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "A6xj724RUiU9",
    "outputId": "dfa89569-9306-467e-9429-226988d5a5b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50830 samples, validate on 12708 samples\n",
      "Epoch 1/5\n",
      "50830/50830 [==============================] - 288s 6ms/step - loss: 0.2151 - acc: 0.9324 - val_loss: 0.1175 - val_acc: 0.9644\n",
      "Epoch 2/5\n",
      "50830/50830 [==============================] - 289s 6ms/step - loss: 0.0820 - acc: 0.9737 - val_loss: 0.1114 - val_acc: 0.9667\n",
      "Epoch 3/5\n",
      "50830/50830 [==============================] - 289s 6ms/step - loss: 0.0535 - acc: 0.9834 - val_loss: 0.0607 - val_acc: 0.9840\n",
      "Epoch 4/5\n",
      "50830/50830 [==============================] - 307s 6ms/step - loss: 0.0478 - acc: 0.9852 - val_loss: 0.0869 - val_acc: 0.9786\n",
      "Epoch 5/5\n",
      "50830/50830 [==============================] - 287s 6ms/step - loss: 0.0398 - acc: 0.9881 - val_loss: 0.0992 - val_acc: 0.9754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f780832a710>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the CNN\n",
    "\n",
    "conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "conv_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /home/vismay/Vismay/Softwares/Anaconda3/lib/python3.6/site-packages (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/vismay/Vismay/Softwares/Anaconda3/lib/python3.6/site-packages (from h5py) (1.14.3)\n",
      "Requirement already satisfied: six in /home/vismay/Vismay/Softwares/Anaconda3/lib/python3.6/site-packages (from h5py) (1.11.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# How to save and load models to disk: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = conv_model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "conv_model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Traffic_signs_classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
